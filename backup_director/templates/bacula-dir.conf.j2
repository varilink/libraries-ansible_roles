Director {
  Name = {{ backup_director_name }}
  DIRport = 9101
  QueryFile = "/etc/bacula/scripts/query.sql"
  WorkingDirectory = "/var/lib/bacula"
  PidDirectory = "/run/bacula"
  Password = "{{ backup_director_console_password }}"
  Messages = Daemon
}

#-------------------------------------------------------------------------------
# JobDefs Directives
# - BackupJob = self explanatory
# - CopyJob = copy media to Dropbox for disaster recovery
#-------------------------------------------------------------------------------

JobDefs {
  Name = BackupJob
  Type = Backup
  Storage = File # default for all backup jobs, overriden for external clients
  Messages = Standard
  Priority = 10
}

JobDefs {
  Name = CopyJob
  Type = Copy
  Storage = FileCopy
  Messages = Standard
  # lower priority than BackupJob so that backups complete before copying media
  Priority = 20
  Pool = DailyOnSite
  FileSet = "Full Set"
  Selection Type = PoolUncopiedJobs
}

#-------------------------------------------------------------------------------
# Job Directives
# - Backup jobs for each indivdual server
# - Backup job for the catalog
# - Copy job to copy catalog backup media to Dropbox
# - Copy job to copy server backup media to Dropbox for on-site servers
# - Admin job that reports the status of Dropbox synchronisation on the director
# - Standard restore template provided by the vendor
#-------------------------------------------------------------------------------
{% for hostname in hostnames %}

Job {
  Name = {{ hostname }}
  JobDefs = BackupJob
  Client = {{ hostname }}
  Write Bootstrap = "/var/lib/bacula/Dropbox/{{ backup_copy_folder }}/%n.bsr"
{#-
Use a FileSet that is specific to each server to be backed up and is in fact
derived from the Ansible roles that are deployed to each server.
#}

  FileSet = {{ hostname }}
  {%- if hostname in groups['external'] %}

{#-
If the server is off-site then we use the schedule and default pool for backups
of off-site servers instead. We also use the Storage directive that is
configured to accept connections from outside of the office network for the
storage director that is located on the office network.
#}

    {%- if  backup_director_schedules_active %}

  Schedule = DailyOffSite

    {%- endif %}

  Pool = DailyOffSite

  {%- elif hostname in groups['internal'] %}

{#-
If the server is on-site then use the schedule and default pool for backups of
on-site servers. The schedule overrides the default pool for full backups weekly
and monthly. We can use the default Storage directive for the BackupJob JobDefs
directive, since both the server being backed up and the storage director are
both on the office network.
#}

    {%- if  backup_director_schedules_active %}

  Schedule = DailyOnSite

    {%- endif %}

  Pool = DailyOnSite

  {%- endif %}{# hostname in groups['external'] #}

  {#-
  Use an array variable "roles" to capture all the roles that are applicable to
  the current host.
  #}

  {%- set roles = [] %}

  {%- include 'includes/roles.j2' %}

  {%- if 'backup_storage' in roles %}

  Client Run After Job = "/etc/bacula/scripts/check-storage-dropbox-status.sh"
  {%- endif %}

}
{% endfor %}{# hostname in hostnames #}

Job {
  Name = BackupCatalog
  JobDefs = BackupJob
  Client = {{ inventory_hostname }}
  FileSet = Catalog
  Level = Full
  Pool = Catalog
{% if  backup_director_schedules_active %}
  Schedule = Catalog
{% endif %}
  # This script provided by the vendor creates an ASCII copy of the catalog
  # Arguments to make_catalog_backup.pl are:
  #  make_catalog_backup.pl <catalog-name>
  RunBeforeJob = "/etc/bacula/scripts/make_catalog_backup.pl MyCatalog"
  # Script provided by vendor that deletes the copy of the catalog
  RunAfterJob  = "/etc/bacula/scripts/delete_catalog_backup"
  # Run after the other backup jobs but before the copy jobs
  Priority = 15
}

Job {
  Name = DailyCatalogCopy
  JobDefs = CopyJob
  # Has no effect for this job but Client is mandatory for Job directives
  Client = {{ inventory_hostname }}
  Pool = Catalog
{% if  backup_director_schedules_active %}
  Schedule = DailyCatalogCopy
{% endif %}
}

Job {
  Name = DailyCopy
  JobDefs = CopyJob
  # Has no effect for this job but Client is mandatory for Job directives
  Client = {{ inventory_hostname }}
  Pool = DailyCopy
{% if  backup_director_schedules_active %}
  Schedule = DailyCopy
{% endif %}
}

Job {
  Name = CheckDropboxStatus
  Type = Admin
  Messages = Standard
  Priority = 10
  # Attributes that are mandatory for a Job but irrelevant for this Job Type
  Client = {{ inventory_hostname }}
  FileSet = "Full Set"
  Pool = DailyOnSite
  Storage = File
  Run Before Job = "/etc/bacula/scripts/check-director-dropbox-status.sh"
}

#
# Standard Restore template, to be changed by Console program
#  Only one such job is needed for all Jobs/Clients/Storage ...
#
Job {
  Name = "Restore Files"
  Type = Restore
  Client={{ inventory_hostname }}
  Storage = File
# The FileSet and Pool directives are not used by Restore Jobs
# but must not be removed
  FileSet="Full Set"
  Pool = DailyOnSite
  Messages = Standard
  Where = /tmp/bacula-restores
}

#-------------------------------------------------------------------------------
# FileSet Directives
# - FileSet directives for each individual server generated by Ansible.
# - Vendor provided FileSet for the catalog backup.
# - Vendor provided standard "Full Set" FileSet, which we use as a default
#   entry where we have to specify a default FileSet but we never actually use
#   it.
#-------------------------------------------------------------------------------
{% for hostname in hostnames %}

FileSet {
  Name = Catalog

  # Client specific fileset for {{ hostname }}
  Name = {{ hostname }}
  Ignore FileSet Changes = yes

  # Catch-all files to include. These will be progressively replaced by role and
  # client files to include from the client specific filesets. According to the
  # Bacula documenation, if a file or directory is duplicated then that will
  # lead to a repeat backup; for example '/' and 'usr'. The documentation
  # doesn't say whether the same is true of an exact duplication. However, this
  # makes it important that our FileSet definitions are as targeted as possible.

  Include {
    Options {
      signature = MD5
    }
    File = "/var/lib/bacula/bacula.sql"

    # Home folders excluding mail directories
    File = /home
    ExcludeDirContaining = maildirfolder

  }

  Exclude {

    # Exclusions in /home that we don't need
    File = /home/*/.ansible
    File = /home/*/.cache
    File = /home/osmc

  }

  Include {

    Options {
      Compression=GZIP
      Signature = MD5
    }

    # APT sources
    File = /etc/apt/sources.list
    File = /etc/apt/sources.list.d

    # Exim MTA
    File = /etc/exim4

    # Filesystem table
    File = /etc/fstab

    # Cron jobs
    File = /etc/crontab
    File = /etc/cron.daily
    File = /etc/cron.hourly
    File = /etc/cron.monthly
    File = /etc/cron.weekly

    # Network
    File = /etc/hosts
    File = /etc/network/interfaces
    File = /etc/resolv.conf

    # Var localisations
    File = /var/local

  }

  Include {

    # User localisations
    File = /usr/local

    Options {
      Compression=GZIP
      Signature = MD5
    }

    # Exclude /usr/local/share/nginx using a WildDir option here as we want to
    # include specific directories within this path in relevant roles.
    Options {
      Exclude = yes
      WildDir = "/usr/local/share/nginx"
    }

  }

  Exclude {

    File = /usr/local/share/man
    File = /usr/local/share/perl
    File = /usr/local/lib/python3.5
    File = /usr/local/lib/python3.9
    File = /usr/local/lib/arm-linux-gnueabihf/perl

  }

  {#-
  Use an array variable "roles" to capture all the roles that are applicable to
  the current host.
  #}

  {%- set roles = [] %}

  {% include 'includes/roles.j2' %}

# Include files for each role deployed to this host for which a file is provided
  {%- for role in roles | unique | sort %}
    {%- for include_file in include_files %}
      {%-
        if (
          include_file.path |
          regex_search(role + '/files/bacula-fileset\.conf$')
        )
      %}

@/etc/bacula/role-filesets/{{ role }}.conf
      {%- endif %}
    {%- endfor %}{# include_file in include_files #}
  {%- endfor %}{# role in roles | unique | sort #}


}

FileSet {
  Include {
    Options {
      signature = MD5
    }
  }
}

FileSet {
  Name = "Full Set"
  Include {
    Options {
      signature = MD5
    }
#
#  Put your list of files here, preceded by 'File =', one per line
#    or include an external list with:
#
#    File = <file-name
#
#  Note: / backs up everything on the root partition.
#    if you have other partitions such as /usr or /home
#    you will probably want to add them too.
#
#  By default this is defined to point to the Bacula binary
#    directory to give a reasonable FileSet to backup to
#    disk storage during initial testing.
#
    File = /usr/sbin
  }

#
# If you backup the root directory, the following two excluded
#   files can be useful
#
  Exclude {
    File = /var/lib/bacula
    File = /nonexistant/path/to/file/archive/dir
    File = /proc
    File = /tmp
    File = /sys
    File = /.journal
    File = /.fsck
  }
}

#-------------------------------------------------------------------------------
# Schedules
# - Daily backup of the catalog.
# - Daily copy of the catalog backup media to Dropbox.
# - Daily backup of off-site servers, with media drawn from pools for
#   incremental normal backup and full backups weekly and monthly.
# - Daily backup of on-site servers, with media drawn from pools for incremental
#   normal backup and full backups weekly and monthly.
#-------------------------------------------------------------------------------

Schedule {
  Name = Catalog
  Run = at 01:10
}

Schedule {
  Name = DailyCatalogCopy
  Run = at 07:05
}

Schedule {
  Name = DailyOffSite
  Run = Level=Full Pool=MonthlyOffSite 1st sat at 01:05
  Run = Level=Full Pool=WeeklyOffSite 2nd-5th sat at 01:05
  Run = Level=Incremental Pool=DailyOffSite sun-fri at 01:05
}

# Daily backup of servers located on the internal office network
Schedule {
  Name = DailyOnSite
  Run = Level=Full Pool=MonthlyOnSite 1st sat at 01:05
  Run = Level=Full Pool=WeeklyOnSite 2nd-5th sat at 01:05
  Run = Level=Incremental Pool=DailyOnSite sun-fri at 01:05
}

Schedule {
  Name = DailyCopy
  Run = Pool= DailyOnSite sun-fri at 02:35
}

#-------------------------------------------------------------------------------
# Client Directives
#-------------------------------------------------------------------------------

{% for hostname in hostnames %}
Client {
  Name = {{ hostname }}
  Address = {{ hostname }}
  FDPort = 9102
  Catalog = MyCatalog
  Password = "{{ hostvars[hostname]['backup_client_director_password'] }}"
  File Retention = 60 days
  Job Retention = 6 months
  AutoPrune = yes
}

{% endfor %}
#-------------------------------------------------------------------------------
# Storage Directives
#-------------------------------------------------------------------------------

Storage {
  Name = File
  Address = "{{ backup_storage_host }}"
  SDPort = 9103
  Password = "{{ backup_storage_password }}"
  Device = FileStorage
  Media Type = File
}

Storage {
   Name = FileCopy
   Address = "{{ backup_storage_host }}"
   SDPort = 9103
   Password = "{{ backup_storage_password }}"
   Device = FileStorageCopy
   Media Type = File
}

# Generic catalog service
Catalog {
  Name = MyCatalog
  dbname = "bacula";
  DB Address = "{{ backup_database_host }}";
  dbuser = "{{ backup_database_username }}";
  dbpassword = "{{ backup_database_password }}"
}

# Reasonable message delivery -- send most everything to email address
#  and to the console
Messages {
  Name = Standard
#
# NOTE! If you send to two email or more email addresses, you will need
#  to replace the %r in the from field (-f part) with a single valid
#  email address in both the mailcommand and the operatorcommand.
#  What this does is, it sets the email address that emails would display
#  in the FROM field, which is by default the same email as they're being
#  sent to.  However, if you send email to more than one address, then
#  you'll have to set the FROM address manually, to a single address.
#  for example, a 'no-reply@mydomain.com', is better since that tends to
#  tell (most) people that its coming from an automated source.

#
  mailcommand = "/usr/sbin/bsmtp -h localhost -f \"\(Bacula\) \<%r\>\" -s \"Bacula: %t %e of %c %l\" %r"
  operatorcommand = "/usr/sbin/bsmtp -h localhost -f \"\(Bacula\) \<%r\>\" -s \"Bacula: Intervention needed for %j\" %r"
  mail = root = all, !skipped
  operator = root = mount
  console = all, !skipped, !saved
#
# WARNING! the following will create a file that you must cycle from
#          time to time as it will grow indefinitely. However, it will
#          also keep all your messages if they scroll off the console.
#
  append = "/var/log/bacula/bacula.log" = all, !skipped
  catalog = all
}


#
# Message delivery for daemon messages (no job).
Messages {
  Name = Daemon
  mailcommand = "/usr/sbin/bsmtp -h localhost -f \"\(Bacula\) \<%r\>\" -s \"Bacula daemon message\" %r"
  mail = root = all, !skipped
  console = all, !skipped, !saved
  append = "/var/log/bacula/bacula.log" = all, !skipped
}

#-------------------------------------------------------------------------------
# Pool Directives
#-------------------------------------------------------------------------------

Pool {
  # Pool for daily catalog backup files
  # Storage set in JobDefs/Job and NOT here.
  Name = Catalog
  Pool Type = Backup
  Label Format = Catalog
  Maximum Volume Jobs = 1
  AutoPrune = yes
  Recycle = yes
  VolumeRetention = 3d
}

Pool {
  # Pool for daily backup files for on-site (office network) clients.
  # Storage set in JobDefs/Job and NOT here.
  Name = DailyOnSite
  Pool Type = Backup
  Label Format = DailyOnSite
  Maximum Volume Bytes = 5G
  Maximum Volume Jobs = 1
  AutoPrune = yes
  Recycle = yes
  Volume Retention = 10d
  Next Pool = DailyCopy
}

Pool {
  # Pool for weekly backup files for on-site (office network) clients.
  # Storage set in JobDefs/Job and NOT here.
  Name = WeeklyOnSite
  Pool Type = Backup
  Label Format = WeeklyOnSite
  Maximum Volume Bytes = 5G
  Maximum Volume Jobs = 1
  AutoPrune = yes
  Recycle = yes
  VolumeRetention = 30d
  Next Pool = WeeklyCopy
}

Pool {
  # Pool for daily backup files for off-site (Internet) clients.
  # Storage set in JobDefs/Job and NOT here.
  Name = DailyOffSite
  Pool Type = Backup
  Label Format = DailyOffSite
  Maximum Volume Bytes = 5G
  Maximum Volume Jobs = 1
  AutoPrune = yes
  Recycle = yes
  Volume Retention = 10d
}

Pool {
  # Pool for weekly backup files for off-site (Internet) clients.
  # Storage set in JobDefs/Job and NOT here.
  Name = WeeklyOffSite
  Pool Type = Backup
  Label Format = WeeklyOffSite
  Maximum Volume Bytes = 5G
  Maximum Volume Jobs = 1
  AutoPrune = yes
  Recycle = yes
  VolumeRetention = 30d
}

Pool {
  # Pool for monthly backup files for off-site (Internet) clients.
  # Storage set in JobDefs/Job and NOT here.
  Pool Type = Backup
  Name = MonthlyOffSite
  Label Format = MonthlyOffSite
  Maximum Volume Bytes = 5G
  Maximum Volume Jobs = 1
  AutoPrune = yes
  Recycle = yes
  # We make a monthly backup on the 1st Saturday of every month.
  # We want to retain the last three monthly backups.
  #
  # Consider this scenario:
  # 1. Monthly backup runs on Saturday 2nd February;
  # 2. Monthly backup runs on Saturday 2nd March;
  # 3. Monthly backup runs on Saturday 6th April;
  # 4. Monthly backup runs on Saturday 4th May.
  #
  # The run on the 4th May should reuse the volumes written on 2nd February,
  # which was 31 weeks or 91 days previous.
  # Volume retention period is set to 90 days.
  VolumeRetention = 90d
}

Pool {
  # Pool for monthly backup files for on-site (office network) clients.
  # Storage set in JobDefs/Job and NOT here.
  Pool Type = Backup
  Name = MonthlyOnSite
  Label Format = MonthlyOnSite
  Maximum Volume Bytes = 5G
  Maximum Volume Jobs = 1
  AutoPrune = yes
  Recycle = yes
  # We make a monthly backup on the 1st Saturday of every month.
  # We want to retain the last three monthly backups.
  #
  # Consider this scenario:
  # 1. Monthly backup runs on Saturday 2nd February;
  # 2. Monthly backup runs on Saturday 2nd March;
  # 3. Monthly backup runs on Saturday 6th April;
  # 4. Monthly backup runs on Saturday 4th May.
  #
  # The run on the 4th May should reuse the volumes written on 2nd February,
  # which was 31 weeks or 91 days previous.
  # Volume retention period is set to 90 days.
  VolumeRetention = 90d
  Next Pool = WeeklyCopy
}

Pool {
  # Pool for off-site copies of daily backups of on-site servers
  Name = DailyCopy
  Pool Type = Copy
  Label Format = DailyCopy
  Maximum Volume Bytes = 5G
  Maximum Volume Jobs = 1
  AutoPrune = yes
  Recycle = yes
  Volume Retention = 10d
}

Pool {
  # Pool for off-site copies of weekly backups of on-site servers
  Name = WeeklyCopy
  Pool Type = Copy
  Label Format = WeeklyCopy
  Maximum Volume Bytes = 5G
  Maximum Volume Jobs = 1
  AutoPrune = yes
  Recycle = yes
  VolumeRetention = 6d
}

#
# Restricted console used by tray-monitor to get the status of the director
#
Console {
  Name = {{ backup_monitor_name }}
  Password = "{{ backup_director_monitor_password }}"
  JobACL = {% for hostname in hostnames %}{{ hostname }}, {% endfor %}
BackupCatalog, DailyCatalogCopy, DailyCatalogCopy
  ClientACL = {% for hostname in hostnames %}{% if loop.index != 1 %}
, {% endif %}{{ hostname }}{% endfor %}

  StorageACL = File, FileCopy
{% if  backup_director_schedules_active %}
  ScheduleACL = Catalog, DailyCatalogCopy, DailyOffSite, DailyOnSite, DailyCopy
{% endif %}
  PoolACL = Catalog, DailyOnSite, WeeklyOnSite, MonthlyOnSite, DailyOffSite, WeeklyOffsite, MonthlyOffite, CatalogCopy, DailyCopy, WeeklyCopy
  FileSetACL = {% for hostname in hostnames %}{{ hostname }}, {% endfor %}
Catalog, "Full Set"
  CatalogACL = MyCatalog
  CommandACL = status, .status
}
